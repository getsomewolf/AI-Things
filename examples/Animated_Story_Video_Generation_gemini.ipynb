{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA6uYJYR5xLY"
      },
      "source": [
        "by [Yousif Ahmed](https://www.linkedin.com/in/yousif-hag-ahmed/) .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Gemini API: Animated Story Video Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Animated_Story_Video_Generation_gemini.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "This Colab Notebook demonstrates how to generate an animated story video by:\n",
        "\n",
        "1. Generating a story sequence using structured Google Gemini API (for character consistency).\n",
        "2. Generating images for each scene using Googleâ€™s Imagen API.\n",
        "3. Synthesizing narration audio using Gemini Live.\n",
        "4. Creating short video clips (image + audio overlay) for each scene.\n",
        "5. Combining all clips into one final video.\n",
        "6. Cleaning up temporary files after processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AQJjzmYgH3sX",
        "outputId": "5eae23ea-1adc-47f2-ca72-6872827e2746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: pydantic<3.0.0dev,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.10.6)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.0dev,>=13.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (14.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0dev,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.12.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (4.9)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2025.1.31)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install google-genai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iQSKjF5WH5N9"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named GOOGLE_API_KEY. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/22859a9f498b6de9d8fe4a33161bcdd3f713d5e8/quickstarts/Authentication.ipynb) for an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wltbMJLIIXGk"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iipaJJsBeuBC"
      },
      "source": [
        "# Animated Story Video Generation Steps\n",
        "\n",
        "**1. Story Generation:**\n",
        "\n",
        "*   **Google Gemini API** is used to generate a sequence of story scenes. Each scene includes an image prompt, audio narration text, and character descriptions. However,to ensure consistency in storytelling and character representation,[Gemini is constrained](https://ai.google.dev/gemini-api/docs/structured-output?lang=python) to respond with JSON,a structured data format.\n",
        "*   **User Input:** The user provides a theme and the desired number of scenes for their story. This information guides the story generation process.\n",
        "\n",
        "**2. Image Generation:**\n",
        "\n",
        "*   **Google Imagen API:** Based on the image prompts generated in the previous step, [Imagen generates images](https://ai.google.dev/gemini-api/docs/imagen) for each scene, bringing the story to life visually. This ensures visually appealing animations with a consistent art style.\n",
        "\n",
        "**3. Audio Generation:**\n",
        "\n",
        "*   **Google Gemini Live API:** The narration or dialogue for each scene, generated in the story sequence, is converted to audio using the[ Gemini Live API](https://ai.google.dev/gemini-api/docs/multimodal-live). This adds a voiceover to the animated video, enhancing storytelling.\n",
        "\n",
        "**4. Video Composition:**\n",
        "\n",
        "*   [**MoviePy:**](https://zulko.github.io/moviepy/) This open-source Python library is used to combine the generated images and audio to create short video clips for each scene. It is also used to stitch these clips together into a final animated video.\n",
        "\n",
        "**5. Final Output:**\n",
        "\n",
        "*   The notebook produces an animated video in MP4 format, bringing together all the elements generated in previous steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GrX47ZZejZh"
      },
      "source": [
        "# **Installation and Setup Commands**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EZ_IJv0VegO6",
        "outputId": "ed260109-e514-492d-89fd-73dead474fad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Generating locales (this might take a while)...\n",
            "  en_US.UTF-8... done\n",
            "Generation complete.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq && apt-get install -qq locales\n",
        "!locale-gen en_US.UTF-8\n",
        "!update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8\n",
        "\n",
        "!apt-get -qq -y install espeak-ng > /dev/null 2>&1\n",
        "!pip install -q google-generativeai moviepy Pillow\n",
        "!pip install -q nest_asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "harQ8ZdpXNBd"
      },
      "source": [
        "## Key Library Explanations:\n",
        "\n",
        "**MoviePy:** A Python library for video editing, enabling tasks such as:\n",
        "\n",
        "* Creating video clips from images (`ImageClip`).\n",
        "* Adding audio to videos (`AudioFileClip`).\n",
        "* Combining multiple clips (`CompositeVideoClip`, `concatenate_videoclips`).\n",
        "* Exporting final videos with proper encoding.\n",
        "\n",
        "**Wave:** A built-in Python library for handling WAV format audio files. It is used in this notebook to:\n",
        "\n",
        "* Process the audio stream received from Google's API.\n",
        "* Save the audio stream as WAV files.\n",
        "\n",
        "**PIL (Python Imaging Library):** A fundamental library for image processing in Python. It's used here to:\n",
        "\n",
        "* Open and manipulate images generated by Google's Imagen API.\n",
        "* Save images in various formats.\n",
        "\n",
        "**Nest_asyncio:** A library that enables the use of asynchronous code (using `async` and `await`) within Jupyter notebooks. This is crucial for:\n",
        "\n",
        "* Real-time audio generation using Google's Live API, which streams audio data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "isCm4c89hD3_",
        "outputId": "72b0dc0e-2625-4a15-e3a7-251ed6ff6fba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Core data processing\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "\n",
        "# Image handling\n",
        "from PIL import Image  # For image processing and manipulation\n",
        "from IPython.display import display\n",
        "\n",
        "# Video and audio processing\n",
        "from moviepy.editor import ImageClip, AudioFileClip, CompositeVideoClip, concatenate_videoclips\n",
        "# MoviePy: Essential for video creation, combining images and audio, and video editing\n",
        "# Generate and display final video\n",
        "import time\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "# Type hints\n",
        "import typing_extensions as typing\n",
        "\n",
        "# Async support for Google API calls\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import asyncio\n",
        "import contextlib\n",
        "import wave  # For WAV audio file handling\n",
        "\n",
        "# Google Generative AI\n",
        "from google import genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnQeb-SmhFh2"
      },
      "source": [
        "# Google Generative Models Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwcnOUA6YGf0"
      },
      "source": [
        "v1alpha because you are using the live api to get audio output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VXXqeYyIhBFm"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "# Using v1alpha for the Live API for audio output. See: https://ai.google.dev/gemini-api/docs/multimodal-live\n",
        "client = genai.Client(http_options= {\n",
        "      'api_version': 'v1alpha'\n",
        "})\n",
        "# Create a client for text generation using Gemini.\n",
        "MODEL = \"models/gemini-2.0-flash-exp\"\n",
        "# Create a client for image generation using Imagen.\n",
        "IMAGE_MODEL_ID = \"imagen-3.0-generate-002\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knBNtFw7hTz1"
      },
      "source": [
        "# SECTION 1: Story Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suDVdo61Z3a1"
      },
      "source": [
        "This code uses Gemini's structured output capabilities to generate story sequences.\n",
        "\n",
        "use structured output (JSON schema) here to:\n",
        "\n",
        "1. Ensure consistent formatting across all generated scenes\n",
        "2. Make it easier to process the output programmatically\n",
        "3. Force the model to maintain specific fields needed for video generation\n",
        "4. Prevent inconsistent responses\n",
        "\n",
        "Each scene includes:\n",
        "- `image_prompt`: short scene description\n",
        "- `audio_text`: dialogue or narration text\n",
        "- `character_description`: detailed character/background hints\n",
        "\n",
        "Furthermore, the code uses TypedDict for type safety and clear documentation of the expected structure, making it easier to maintain and debug the application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OimNe8lHhb4f"
      },
      "outputs": [],
      "source": [
        "# Using structured output to ensure consistent story generation\n",
        "# See: https://ai.google.dev/gemini-api/docs/structured-output?lang=python\n",
        "\n",
        "# Define the structure for each story segment using TypedDict for type safety\n",
        "class StorySegment(typing.TypedDict):\n",
        "    image_prompt: str\n",
        "    audio_text: str\n",
        "    character_description: str\n",
        "\n",
        "# Define the overall story response structure\n",
        "class StoryResponse(typing.TypedDict):\n",
        "    complete_story: list[StorySegment]\n",
        "    pages: int\n",
        "\n",
        "def generate_story_sequence(complete_story: str, pages: int) -> list[StorySegment]:\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL,\n",
        "        contents=f'''you are an animation video producer. Generate a story sequence about {complete_story} in {pages} scenes (with interactions and characters), 1 sec each scene. Write:\n",
        "\n",
        "image_prompt:(define art style for kids animation(consistant for all the characters), no violence) a full description of the scene, the characters in it, and the background in 20 words or less. Progressively shift the scene as the story advances.\n",
        "audio_text: a one-sentence dialogue/narration for the scene.\n",
        "character_description: no people ever, only animals and objects. Describe all characters (consistent names, features, clothing, etc.) with an art style reference (e.g., \"Pixar style,\" \"photorealistic,\" \"Ghibli\") in 30 words or less.\n",
        "''',\n",
        "        config={\n",
        "            'response_mime_type': 'application/json',\n",
        "            'response_schema': list[StoryResponse]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        story_data_text = response.text  # Get the JSON text\n",
        "        story_data_list = json.loads(story_data_text)\n",
        "        if isinstance(story_data_list, list) and len(story_data_list) > 0:\n",
        "            story_data = story_data_list[0]\n",
        "            return story_data.get('complete_story', []), story_data.get('character_description', {})\n",
        "        else:\n",
        "            return []\n",
        "    except (KeyError, TypeError, IndexError, json.JSONDecodeError) as e:\n",
        "        print(f\"Error parsing JSON: {e}\")\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgL_I6hoq2cK"
      },
      "source": [
        "Example Usage: Define a Theme and Generate Scenes\n",
        "Write your own theme and number of scenes to play and experiment with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VzJhnJcwq1Rq",
        "outputId": "8420ed00-87cf-47f4-aee4-7e3d0df034ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Story Segments:\n",
            "[\n",
            "  {\n",
            "    \"image_prompt\": \"Cartoon cat jumps at a squeaky toy in a sunny living room. Bookshelf, couch, and window in background, kids animation style.\",\n",
            "    \"audio_text\": \"Whiskers pounces on the squeaky mouse!\",\n",
            "    \"character_description\": \"Whiskers: Orange tabby cat, big green eyes, playful expression, kids animation style. Wears a blue collar with a bell. Pixar style.\"\n",
            "  },\n",
            "  {\n",
            "    \"image_prompt\": \"Cartoon dog chases a ball outside, cat watches from window. Green grass and blue sky background, kids animation style.\",\n",
            "    \"audio_text\": \"Barnaby fetches the ball, while Whiskers watches from inside.\",\n",
            "    \"character_description\": \"Barnaby: Golden retriever puppy, wagging tail, excited expression, kids animation style. Red collar. Pixar style.\"\n",
            "  },\n",
            "  {\n",
            "    \"image_prompt\": \"Cat and dog sleeping together on a soft blanket indoors. Night scene, starry sky seen through window, kids animation style.\",\n",
            "    \"audio_text\": \"Best friends, Whiskers and Barnaby, snuggle and sleep soundly.\",\n",
            "    \"character_description\": \"Whiskers: Orange tabby cat, sleeping peacefully. Barnaby: Golden retriever puppy, curled up next to the cat. Pixar style.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "#@title Generate Story Segments\n",
        "theme = \"a cat and a dog playing \" #@param {type:\"string\"}\n",
        "num_scenes = 3 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "story_segments, _ = generate_story_sequence(theme, num_scenes)\n",
        "print(\"\\nGenerated Story Segments:\")\n",
        "print(json.dumps(story_segments, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3RZNG0ehiOG"
      },
      "source": [
        "# SECTION 2: Image and Audio Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f1Ko_e8VR395"
      },
      "outputs": [],
      "source": [
        "@contextlib.contextmanager\n",
        "def wave_file(filename, channels=1, rate=24000, sample_width=2):\n",
        "    with wave.open(filename, \"wb\") as wf:\n",
        "        wf.setnchannels(channels)\n",
        "        wf.setsampwidth(sample_width)\n",
        "        wf.setframerate(rate)\n",
        "        yield wf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7ala2K5ih9jW"
      },
      "outputs": [],
      "source": [
        "# --- Cell 2: Definitions and setup ---\n",
        "temp_audio_files = []  # To track temporary audio files\n",
        "temp_image_files = []  # To track temporary image files\n",
        "video_clips = []       # To store individual video clips for each scene\n",
        "\n",
        "def generate_audio_live(api_text, output_filename):\n",
        "    import asyncio\n",
        "    collected_audio = bytearray()\n",
        "\n",
        "    async def _generate():\n",
        "        config = {\n",
        "            \"generation_config\": {\"response_modalities\": [\"AUDIO\"]}\n",
        "        }\n",
        "        # Connect to the Live API using the client already initialized above.\n",
        "        async with client.aio.live.connect(model=MODEL, config=config) as session:\n",
        "            # Send the audio_text prompt; mark as end_of_turn.\n",
        "            await session.send(input=api_text, end_of_turn=True)\n",
        "            # Collect audio data as it streams in.\n",
        "            async for response in session.receive():\n",
        "                if response.data:\n",
        "                    collected_audio.extend(response.data)\n",
        "        return bytes(collected_audio)\n",
        "\n",
        "    # Run the async function and collect the audio bytes.\n",
        "    audio_bytes = asyncio.run(_generate())\n",
        "    # Write the collected audio bytes into a WAV file using the helper.\n",
        "    with wave_file(output_filename) as wf:\n",
        "        wf.writeframes(audio_bytes)\n",
        "    return output_filename\n",
        "\n",
        "\n",
        "\n",
        "# Note: Use a system instruction to prevent common AI responses and ensure natural narration\n",
        "audio_negative_prompt = \"dont say OK , I will do this or that, just only read this story using voice expressions without introductions or ending ,more segments are comming ,dont say OK , I will do this or that:\\n\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "N2APG_rreYuQ",
        "outputId": "58688e7b-1b03-4414-c9f3-510a979a3ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing scene 0:\n",
            "Image Prompt: Cartoon cat jumps at a squeaky toy in a sunny living room. Bookshelf, couch, and window in background, kids animation style.\n",
            "Audio Text: Whiskers pounces on the squeaky mouse!\n",
            "Character Description: Whiskers: Orange tabby cat, big green eyes, playful expression, kids animation style. Wears a blue collar with a bell. Pixar style.\n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Imagen API is only accessible to billed users at this time.', 'status': 'INVALID_ARGUMENT'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3f1eb76afe33>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcombined_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"detailed children book animation style \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage_prompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchar_desc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     result = client.models.generate_images(\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_MODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombined_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_images\u001b[0;34m(self, model, prompt, config)\u001b[0m\n\u001b[1;32m   4050\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4052\u001b[0;31m     response_dict = self._api_client.request(\n\u001b[0m\u001b[1;32m   4053\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4054\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     )\n\u001b[0;32m--> 455\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    388\u001b[0m       )\n\u001b[1;32m    389\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_unauthorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m   def _request_unauthorized(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_unauthorized\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     )\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m     return HttpResponse(\n\u001b[1;32m    415\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Imagen API is only accessible to billed users at this time.', 'status': 'INVALID_ARGUMENT'}}"
          ]
        }
      ],
      "source": [
        "# --- Cell 3: Main processing loop ---\n",
        "for i, segment in enumerate(story_segments):\n",
        "    # Retrieve details for the current scene.\n",
        "    image_prompt = segment['image_prompt']\n",
        "    audio_text =  audio_negative_prompt + segment['audio_text']\n",
        "    audio_text_prompt = segment['audio_text']\n",
        "    char_desc = segment['character_description']\n",
        "    print(f\"Processing scene {i}:\")\n",
        "    print(\"Image Prompt:\", image_prompt)\n",
        "    print(\"Audio Text:\", audio_text_prompt)\n",
        "    print(\"Character Description:\", char_desc)\n",
        "    print(\"--------------------------------\")\n",
        "\n",
        "    # -------------------------\n",
        "    # Image Generation using Google Imagen\n",
        "    # -------------------------\n",
        "    combined_prompt = \"detailed children book animation style \" + image_prompt + \" \" + char_desc\n",
        "\n",
        "    result = client.models.generate_images(\n",
        "        model=IMAGE_MODEL_ID,\n",
        "        prompt=combined_prompt,\n",
        "        config={\n",
        "            \"number_of_images\": 1,\n",
        "            \"output_mime_type\": \"image/jpeg\",\n",
        "            \"person_generation\": \"DONT_ALLOW\",\n",
        "            \"aspect_ratio\": \"1:1\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        if not result.generated_images:\n",
        "            raise ValueError(\"No images were generated. The prompt might have been flagged as harmful. Please modify your prompt and try again.\")\n",
        "        for generated_image in result.generated_images:\n",
        "            image = Image.open(BytesIO(generated_image.image.image_bytes))\n",
        "    except Exception as e:\n",
        "        print(\"Image generation failed \", e)\n",
        "\n",
        "    image_path = f\"image_{i}.png\"\n",
        "    image.save(image_path)\n",
        "    temp_image_files.append(image_path)\n",
        "    display(image)\n",
        "\n",
        "    # -------------------------\n",
        "    # Audio Generation using Google Live API\n",
        "    # -------------------------\n",
        "    audio_path = f\"audio_{i}.wav\"\n",
        "    audio_path = generate_audio_live(audio_text, audio_path)\n",
        "    temp_audio_files.append(audio_path)\n",
        "\n",
        "\n",
        "    # -------------------------\n",
        "    # Create Video Clip (Image + Audio)\n",
        "    # -------------------------\n",
        "    audio_clip = AudioFileClip(audio_path)\n",
        "\n",
        "    # Convert PIL Image to numpy array\n",
        "    np_image = np.array(image)\n",
        "\n",
        "    # Create ImageClip (size is inferred from np_image)\n",
        "    image_clip = ImageClip(np_image).set_duration(audio_clip.duration)\n",
        "\n",
        "    # Store composite clip with audio in memory\n",
        "    composite_clip = CompositeVideoClip([image_clip]).set_audio(audio_clip)\n",
        "    video_clips.append(composite_clip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y13_WC1rh_zk"
      },
      "source": [
        "# SECTION 3: Final Video Assembly and Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pVXq-rjvegMa",
        "outputId": "a9212c64-8454-4425-b321-e1b408037d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "max() arg is an empty sequence",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0b597fb70f88>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate_videoclips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_clips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{int(time.time())}_output_video.mp4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Writing final video to\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfinal_video\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/moviepy/video/compositing/concatenate.py\u001b[0m in \u001b[0;36mconcatenate_videoclips\u001b[0;34m(clips, method, transition, bg_color, ismask, padding)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclips\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
          ]
        }
      ],
      "source": [
        "final_video = concatenate_videoclips(video_clips)\n",
        "output_filename = f\"{int(time.time())}_output_video.mp4\"\n",
        "print(\"Writing final video to\", output_filename)\n",
        "final_video.write_videofile(output_filename, fps=24)\n",
        "\n",
        "# Display the video in the notebook\n",
        "def show_video(video_path):\n",
        "    \"\"\"Display video in notebook\"\"\"\n",
        "    video_file = open(video_path, \"rb\")\n",
        "    video_bytes = video_file.read()\n",
        "    video_b64 = b64encode(video_bytes).decode()\n",
        "    video_tag = f'<video width=\"640\" height=\"480\" controls><source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\"></video>'\n",
        "    return HTML(video_tag)\n",
        "\n",
        "# Show the video\n",
        "display(show_video(output_filename))\n",
        "\n",
        "# Cleanup: Close video clips and remove temporary files\n",
        "final_video.close()\n",
        "for clip in video_clips:\n",
        "    clip.close()\n",
        "for file in temp_audio_files:\n",
        "    os.remove(file)\n",
        "for file in temp_image_files:\n",
        "    os.remove(file)\n",
        "\n",
        "\n",
        "# A video player will appear below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6OjO-kiiNQl"
      },
      "source": [
        "## Final Notes\n",
        "\n",
        "This notebook is designed to run without modifications. It generates an animated story video using multiple Google APIs and open source libraries. Make sure to have a valid API key and to install all the necessary dependencies before running the notebook in Google Colab.\n",
        "\n",
        "\n",
        "If you like to view premade videos using this concept ,Checkout this [website](https://www.vastai.ai/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4677dd58e9b5"
      },
      "source": [
        "## Next Steps\n",
        "### Useful API References:\n",
        "\n",
        "* Learn more about [Structured Outputs](https://ai.google.dev/gemini-api/docs/structured-outputs) in the docs.\n",
        "\n",
        "* [Imagen Pricing](https://ai.google.dev/pricing#2_0flash)\n",
        "\n",
        "* [Imagen Prompt Guide](https://ai.google.dev/gemini-api/docs/imagen-prompt-guide)\n",
        "\n",
        "### Related Examples\n",
        "\n",
        "* [Get Started with Imagen](../quickstarts/Get_started_imagen.ipynb)\n",
        "* [Book Illustration Generation](../examples/Book_illustration.ipynb)\n",
        "* [Get Started with Live API](../quickstarts/Get_started_LiveAPI_tools.ipynb)\n",
        "\n",
        "### Continue Your Discovery of the Gemini API\n",
        "\n",
        "Check out other great Gemini features:\n",
        "\n",
        "* [Video Understanding](../quickstarts/Video_understanding.ipynb)\n",
        "* [Prompting with Video](../quickstarts/Video.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WTwO5zFjvAp"
      },
      "source": [
        "[Contact Contributor](https://github.com/Yousif-GO)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Animated_Story_Video_Generation_gemini.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}